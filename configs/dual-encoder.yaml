# General parameters
project: DEXML # Project name
expname: dexml-exp # Experiment name
desc: 'Transformer based dual encoder training' # Short description of the exepriment
dataset: '' # Dataset name 
data_manager: two-tower # Data manager class identifier
save: True # Save best model and score matrix
resume_path: '' # Resume training from this model checkpoint
data_tokenization: offline # Mode of data tokenization [offline/online]
num_val_points: 0 # Number of validation points taken from training set for evaluation during training
track_metric: nDCG@5 # Track this metric to evaluate best model
transpose_trn_dataset: False
expand_multilabel_dataset: False

# Network parameters
net: dist-de-all # Network class identifier [dist-de-all/dist-de-hnm/dist-clf-net]
tf: distilbert-base-uncased # Name of transformer encoder to be used in the model, all huggingface transformer model names are applicable
tf_pooler: cls # Defines how to pool transformer final layer embeddings into a single vector representation [cls/mean]
tf_max_len: 32 # Maximum input sequence length of transformer
bottleneck_dim: 0 # If non-zero then encoder embeddings are embeddings are projected by a linear MLP to specified dimensions
tau: 0.05 # temperature parameter in loss calculation

# Loss parameters
loss: sim-loss # Loss class identifier
loss_sample: True # Sample positive labels for loss calculation
loss_criterion: decoupled-softmax # Loss criterion class identifier [decoupled-softmax/topk/softmax/bce]
loss_reduction: mean # reduction over data-points in the mini-batch [mean/sum]
loss_weighted: False # Weight loss by non-binary relevance score
## Topk parameters (when using with loss_criterion: topk)
topk_K: 5 # K value for topk operation
topk_alpha: 2 # applies topk using sigmoid(alpha * x) where x is the raw score
topk_n_iter: 32 # Number of iterations for binary search in topk iteration

# Negative mining parameters
neg_type: none # Negative mining type [none/shorty/in-batch]
hard_neg_start: 1000 # Start hard negative mining after this epoch
hard_neg_topk: 100 # Sample hard negatives from topk shortlist
num_neg_samples: 1 # Number of negative samples
num_pos_samples: 1 # Number of positive samples
# NOTE: hard negative mining shortlist is updated every `eval_interval` (which by default=5) epochs

# Training parameters
optim_bundle: base # Optimizer bundle class identifier
optim: adamw # Torch optimizer class identifier
num_epochs: 100 # Number of epochs
dropout: 0 # Dropout on encoder embeddings
warmup: 0.1 # Fraction of warmup steps
bsz: 1000 # Mini-batch size
gc_bsz: 1000 # batch size for gradient cache building while encoding labels
eval_interval: 5 # Evaluate current model at every specified number of epochs
eval_topk: 100 # Number of label predictions for each test point during evaluation
lr: 1.0e-4 # Learning rate for the rest of the parameters
weight_decay: 0.01 # Optimizer weight decay
amp_encode: False # Encode input (i.e. transformer forward) using pytorch amp
norm_embs: True # Normalize embeddings